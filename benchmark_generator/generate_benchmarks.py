#!/usr/bin/env python3
"""
generate_benchmarks.py

Main entry point for generating ESP32-C6 RISC-V benchmarks.
Imports latency and throughput generators from separate modules.

Usage:
    python benchmark_generator/generate_benchmarks.py \
        --input isa_extraction/output/esp32c6_instructions.json \
        --output esp32c6_benchmark/main/generated_benchmarks.h

    # Latency only (original behavior)
    python benchmark_generator/generate_benchmarks.py --latency-only

    # Throughput only
    python benchmark_generator/generate_benchmarks.py --throughput-only
"""

import argparse
import json
import sys
from datetime import datetime
from pathlib import Path
from typing import List, Tuple

# Import from sibling modules using relative imports
# Since this script can be run directly, handle both cases
try:
    from .common import BenchmarkConfig, Instruction, DEFAULT_INPUT, DEFAULT_OUTPUT
    from .latency_generator import LatencyBenchmarkGenerator
    from .throughput_generator import ThroughputBenchmarkGenerator
    from .branch_predictor_generator import BranchPredictorBenchmarkGenerator, BranchPredictorConfig
except ImportError:
    # Running as script directly
    from common import BenchmarkConfig, Instruction, DEFAULT_INPUT, DEFAULT_OUTPUT
    from latency_generator import LatencyBenchmarkGenerator
    from throughput_generator import ThroughputBenchmarkGenerator
    from branch_predictor_generator import BranchPredictorBenchmarkGenerator, BranchPredictorConfig


def load_instructions(input_path: str) -> List[dict]:
    """Load instruction definitions from JSON file."""
    try:
        with open(input_path, 'r') as f:
            return json.load(f)
    except FileNotFoundError:
        print(f"Error: Input file '{input_path}' not found")
        sys.exit(1)
    except json.JSONDecodeError as e:
        print(f"Error: Failed to parse JSON: {e}")
        sys.exit(1)


def generate_header_content(
    latency_functions: List[str],
    throughput_functions: List[str],
    latency_descriptors: List[str],
    throughput_descriptors: List[str],
    config: BenchmarkConfig
) -> str:
    """Generate the complete header file content."""
    timestamp = datetime.now().isoformat()
    
    all_functions = "\n".join(latency_functions + throughput_functions)
    all_descriptors = ",\n".join(latency_descriptors + throughput_descriptors)
    
    total_benchmarks = len(latency_descriptors) + len(throughput_descriptors)
    
    header = f'''/**
 * @file generated_benchmarks.h
 * @brief Auto-generated benchmarks for ESP32-C6 RISC-V instructions
 *
 * Generated: {timestamp}
 * Generator: generate_benchmarks.py
 *
 * DO NOT EDIT THIS FILE MANUALLY - it is auto-generated.
 *
 * Benchmark Types:
 * - Latency: Dependency chain measurement (cycles per instruction)
 * - Throughput: Independent instruction measurement (cycles per instruction)
 *
 * Methodology based on: "uops.info: Characterizing Latency, Throughput, and
 * Port Usage of Instructions on Intel Microarchitectures" (ASPLOS '19)
 */

#ifndef GENERATED_BENCHMARKS_H
#define GENERATED_BENCHMARKS_H

#include "benchmark_interface.h"
#include <limits.h>

/*
 * =============================================================================
 * Benchmark Set Information
 * =============================================================================
 */

const char *BENCHMARK_SET_NAME = "ESP32-C6 Latency+Throughput Benchmarks v2.0";

/*
 * =============================================================================
 * Benchmark Configuration
 * =============================================================================
 */

const benchmark_config_t BENCHMARK_CONFIG = {{
    .warmup_iterations = {config.warmup_iterations},
    .measurement_iterations = {config.measurement_iterations},
    .repeat_count = {config.repeat_count},
    .chain_length = {config.chain_length}
}};

/*
 * =============================================================================
 * Benchmark Function Implementations
 * =============================================================================
 */

{all_functions}

/*
 * =============================================================================
 * Benchmark Descriptor Array ({total_benchmarks} total)
 * - Latency benchmarks: {len(latency_descriptors)}
 * - Throughput benchmarks: {len(throughput_descriptors)}
 * =============================================================================
 */

const benchmark_descriptor_t BENCHMARKS[] = {{
{all_descriptors}
}};

const size_t BENCHMARK_COUNT = sizeof(BENCHMARKS) / sizeof(BENCHMARKS[0]);

#endif /* GENERATED_BENCHMARKS_H */
'''
    return header


def generate_branch_predictor_header(
    functions: List[str],
    descriptors: List[str],
    config: 'BranchPredictorConfig'
) -> str:
    """Generate header file content for branch predictor experiments."""
    timestamp = datetime.now().isoformat()
    
    all_functions = "\n".join(functions)
    all_descriptors = ",\n".join(descriptors)
    
    header = f'''/**
 * @file generated_benchmarks.h
 * @brief Branch Predictor Direction Experiment for ESP32-C6
 *
 * Generated: {timestamp}
 * Generator: generate_benchmarks.py --branch-predictor
 *
 * Experiments to determine if ESP32-C6 uses:
 * - Static BTFN (Backward-Taken, Forward-Not-Taken) prediction
 * - Dynamic BTB (Branch Target Buffer) with learning
 *
 * Expected Results (Static BTFN):
 * - BRANCH_BACKWARD_TAKEN: ~1 cycle (correct prediction)
 * - BRANCH_FORWARD_TAKEN: ~3 cycles (misprediction)
 * - BRANCH_FORWARD_NOT_TAKEN: ~1 cycle (correct prediction)
 * - BRANCH_BACKWARD_NOT_TAKEN: ~3 cycles (misprediction)
 *
 * Expected Results (Dynamic BTB):
 * - All tests: ~3 cycles initially, ~1 cycle after warmup
 */

#ifndef GENERATED_BENCHMARKS_H
#define GENERATED_BENCHMARKS_H

#include "benchmark_interface.h"
#include <limits.h>

/*
 * =============================================================================
 * Benchmark Set Information
 * =============================================================================
 */

const char *BENCHMARK_SET_NAME = "ESP32-C6 Branch Predictor Direction Experiment";

/*
 * =============================================================================
 * Benchmark Configuration
 * =============================================================================
 */

const benchmark_config_t BENCHMARK_CONFIG = {{
    .warmup_iterations = {config.warmup_iterations},
    .measurement_iterations = {config.measurement_iterations},
    .repeat_count = {config.repeat_count},
    .chain_length = {config.loop_iterations}
}};

/*
 * =============================================================================
 * Benchmark Function Implementations
 * =============================================================================
 */

{all_functions}

/*
 * =============================================================================
 * Benchmark Descriptors ({len(descriptors)} tests)
 * =============================================================================
 */

const benchmark_descriptor_t BENCHMARKS[] = {{
{all_descriptors}
}};

const size_t BENCHMARK_COUNT = sizeof(BENCHMARKS) / sizeof(BENCHMARKS[0]);

#endif /* GENERATED_BENCHMARKS_H */
'''
    return header


def main():
    parser = argparse.ArgumentParser(
        description="Generate latency and throughput benchmarks for ESP32-C6 RISC-V instructions"
    )
    parser.add_argument(
        "--input", "-i",
        default=str(DEFAULT_INPUT),
        help=f"Input JSON file with instruction definitions"
    )
    parser.add_argument(
        "--output", "-o",
        default=str(DEFAULT_OUTPUT),
        help=f"Output header file path"
    )
    parser.add_argument(
        "--warmup",
        type=int,
        default=100,
        help="Number of warmup iterations (default: 100)"
    )
    parser.add_argument(
        "--iterations",
        type=int,
        default=1000,
        help="Number of measurement iterations (default: 1000)"
    )
    parser.add_argument(
        "--repeats",
        type=int,
        default=5,
        help="Number of times to repeat measurement (default: 5)"
    )
    parser.add_argument(
        "--chain-length",
        type=int,
        default=100,
        help="Length of dependency chain for latency (default: 100)"
    )
    parser.add_argument(
        "--independent-count",
        type=int,
        default=8,
        help="Number of independent instructions for throughput (default: 8)"
    )
    parser.add_argument(
        "--latency-only",
        action="store_true",
        help="Generate only latency benchmarks"
    )
    parser.add_argument(
        "--throughput-only",
        action="store_true",
        help="Generate only throughput benchmarks"
    )
    parser.add_argument(
        "--category",
        choices=["all", "arithmetic", "multiply", "memory", "control", "atomic"],
        default="all",
        help="Filter by instruction category (default: all)"
    )
    parser.add_argument(
        "--branch-predictor",
        action="store_true",
        help="Generate branch predictor direction experiment benchmarks only"
    )
    parser.add_argument(
        "--loop-iterations",
        type=int,
        default=100,
        help="Branch iterations per measurement for branch predictor tests (default: 100)"
    )
    parser.add_argument(
        "--cache",
        action="store_true",
        help="Generate cache latency benchmarks (SRAM, Flash Hit, Flash Miss)"
    )
    parser.add_argument(
        "--hazards",
        action="store_true",
        help="Generate structural hazard benchmarks (e.g. DIV+ADD)"
    )
    parser.add_argument(
        "--store-buffer",
        action="store_true",
        help="Generate Store Buffer benchmarks (burst, forwarding)"
    )

    args = parser.parse_args()

    # Branch predictor mode: generate specialized experiments only
    if args.branch_predictor:
        print("=== Generating Branch Predictor Direction Experiment ===")
        bp_config = BranchPredictorConfig(
            warmup_iterations=args.warmup,
            measurement_iterations=args.iterations,
            repeat_count=args.repeats,
            loop_iterations=args.loop_iterations
        )
        
        bp_gen = BranchPredictorBenchmarkGenerator(bp_config)
        bp_functions = bp_gen.generate_all_benchmarks()
        bp_descriptors = bp_gen.generate_all_descriptors()
        
        # Generate header with branch predictor tests only
        header_content = generate_branch_predictor_header(
            bp_functions, bp_descriptors, bp_config
        )
        
        with open(args.output, 'w') as f:
            f.write(header_content)
        
        print(f"Generated {len(bp_gen.generated_benchmarks)} branch predictor tests:")
        for name, _ in bp_gen.generated_benchmarks:
            print(f"  - {name}")
        print(f"Output written to: {args.output}")
        return

    # Cache benchmarks mode
    if args.cache:
        print("=== Generating Cache Latency Benchmarks ===")
        try:
            from .cache_benchmarks import CacheBenchmarkConfig, CacheBenchmarkGenerator
        except ImportError:
            from cache_benchmarks import CacheBenchmarkConfig, CacheBenchmarkGenerator

        cache_config = CacheBenchmarkConfig(
            warmup_iterations=args.warmup,
            measurement_iterations=args.iterations,
            repeat_count=args.repeats
        )
        
        cache_gen = CacheBenchmarkGenerator(cache_config)
        c_code, descriptors = cache_gen.generate_all_benchmarks()
        
        timestamp = datetime.now().isoformat()
        
        header = f'''/**
 * @file generated_benchmarks.h
 * @brief Cache Latency Benchmarks for ESP32-C6
 *
 * Generated: {timestamp}
 * Generator: generate_benchmarks.py --cache
 */

#ifndef GENERATED_BENCHMARKS_H
#define GENERATED_BENCHMARKS_H

#include "benchmark_interface.h"
#include <limits.h>
#include <stdlib.h> // For malloc/free

/*
 * =============================================================================
 * Benchmark Set Information
 * =============================================================================
 */

const char *BENCHMARK_SET_NAME = "ESP32-C6 Cache Latency Benchmarks";

/*
 * =============================================================================
 * Benchmark Configuration
 * =============================================================================
 */

const benchmark_config_t BENCHMARK_CONFIG = {{
    .warmup_iterations = {cache_config.warmup_iterations},
    .measurement_iterations = {cache_config.measurement_iterations},
    .repeat_count = {cache_config.repeat_count},
    .chain_length = 0 // Not relevant for cache tests
}};

/*
 * =============================================================================
 * Benchmark Function Implementations
 * =============================================================================
 */

{c_code}

/*
 * =============================================================================
 * Benchmark Descriptors
 * =============================================================================
 */

const benchmark_descriptor_t BENCHMARKS[] = {{
{",\n".join(descriptors)}
}};

const size_t BENCHMARK_COUNT = sizeof(BENCHMARKS) / sizeof(BENCHMARKS[0]);

#endif /* GENERATED_BENCHMARKS_H */
'''
        with open(args.output, 'w') as f:
            f.write(header)
            
        print(f"Generated {len(descriptors)} cache benchmarks to {args.output}")
        return


    # Structural Hazards mode
    if args.hazards:
        print("=== Generating Structural Hazard Benchmarks ===")
        # We reuse the standard throughput generator but only call the hazards method
        # and standard throughput config
        config = BenchmarkConfig(
            warmup_iterations=args.warmup,
            measurement_iterations=args.iterations,
            repeat_count=args.repeats,
            independent_count=args.independent_count
        )
        
        tput_gen = ThroughputBenchmarkGenerator(config)
        
        # Only generate hazards
        c_func, descriptor = tput_gen.generate_structural_hazard_test()
        
        c_functions = [c_func]
        descriptors = [descriptor]
        
        # Generate Header
        header_content = generate_header_content(
            [], # No latency metrics
            c_functions,
            [], # No latency descriptors
            descriptors,
            config
        )
        
        with open(args.output, 'w') as f:
            f.write(header_content)
            
        print(f"Generated 1 structural hazard benchmark to {args.output}")
        return

    # Store Buffer mode
    if args.store_buffer:
        print("=== Generating Store Buffer Benchmarks ===")
        # LatencyBenchmarkGenerator is already imported at module level

        config = BenchmarkConfig(
            warmup_iterations=args.warmup,
            measurement_iterations=args.iterations,
            repeat_count=args.repeats
        )
        
        lat_gen = LatencyBenchmarkGenerator(config)
        c_functions, descriptors = lat_gen.generate_store_buffer_tests()
        
        # Generate Header
        # We can reuse generate_header_content but we need to pass C functions list
        # We'll pass empty lists for instructions we aren't testing
        header_content = generate_header_content(
            [], # No latency metrics
            c_functions,
            [], # No latency descriptors
            descriptors,
            config
        )
        
        with open(args.output, 'w') as f:
            f.write(header_content)
        
        print(f"Generated {len(descriptors)} store buffer benchmarks to {args.output}")
        return

    # Load instructions
    print(f"Loading instructions from: {args.input}")

    instructions_data = load_instructions(args.input)
    print(f"Loaded {len(instructions_data)} instructions")

    # Create configuration
    config = BenchmarkConfig(
        warmup_iterations=args.warmup,
        measurement_iterations=args.iterations,
        repeat_count=args.repeats,
        chain_length=args.chain_length,
        independent_count=args.independent_count
    )

    # Convert to Instruction objects with variants
    instructions = []
    div_mnemonics = {"div", "divu", "rem", "remu"}

    # Low latency setup (Small operands: 20, 4)
    setup_low = """    /* Setup: Low Latency Operands (20, 4) */
    register uint32_t t0_val __asm__("t0") = 20;
    register uint32_t t1_val __asm__("t1") = 4;
    register uint32_t s0_val __asm__("s0") = 20;
    register uint32_t s1_val __asm__("s1") = 4;
    (void)t0_val; (void)t1_val; (void)s0_val; (void)s1_val;"""

    # High latency setup (Large operands: Max, 3)
    setup_high = """    /* Setup: High Latency Operands (Large / Small) */
    register uint32_t t0_val __asm__("t0") = 0xFFFFFFF0;
    register uint32_t t1_val __asm__("t1") = 3;
    register uint32_t s0_val __asm__("s0") = 0xFFFFFFF0;
    register uint32_t s1_val __asm__("s1") = 3;
    (void)t0_val; (void)t1_val; (void)s0_val; (void)s1_val;"""

    for d in instructions_data:
        # Create base instruction
        base_instr = Instruction(
            llvm_enum_name=d["llvm_enum_name"],
            opcode_hex=d.get("opcode_hex", ""),
            test_asm=d["test_asm"],
            latency_type=d.get("latency_type", "unknown"),
            tablegen_entry=d.get("tablegen_entry")
        )
        instructions.append(base_instr)
        
        # Check for variants
        asm = d["test_asm"].lower().strip()
        mnemonic = asm.split()[0] if asm.split() else ""
        
        if mnemonic in div_mnemonics:
            # Low Latency Variant
            low_instr = Instruction(
                llvm_enum_name=d["llvm_enum_name"],
                opcode_hex=d.get("opcode_hex", ""),
                test_asm=d["test_asm"],
                latency_type=d.get("latency_type", "unknown"),
                tablegen_entry=d.get("tablegen_entry"),
                setup_code_override=setup_low,
                name_suffix="_LOW_LAT"
            )
            instructions.append(low_instr)
            
            # High Latency Variant
            high_instr = Instruction(
                llvm_enum_name=d["llvm_enum_name"],
                opcode_hex=d.get("opcode_hex", ""),
                test_asm=d["test_asm"],
                latency_type=d.get("latency_type", "unknown"),
                tablegen_entry=d.get("tablegen_entry"),
                setup_code_override=setup_high,
                name_suffix="_HIGH_LAT"
            )
            instructions.append(high_instr)

    # Category filter mapping
    category_map = {
        "arithmetic": ["arithmetic"],
        "multiply": ["multiply"],
        "memory": ["load", "store", "load_store"],
        "control": ["branch", "jump"],
        "atomic": ["atomic"],
    }

    # Filter by category if specified
    if args.category != "all":
        allowed_types = category_map.get(args.category, [])
        instructions = [i for i in instructions if i.latency_type in allowed_types]
        print(f"Filtered to {len(instructions)} instructions (category: {args.category})")

    latency_functions = []
    throughput_functions = []
    latency_descriptors = []
    throughput_descriptors = []

    # Generate latency benchmarks
    if not args.throughput_only:
        print("\n=== Generating Latency Benchmarks ===")
        lat_gen = LatencyBenchmarkGenerator(config)
        
        for instr in instructions:
            func = lat_gen.generate_benchmark_function(instr)
            if func:
                latency_functions.append(func)
        
        # Generate descriptors
        for instr, func_name in lat_gen.generated_benchmarks:
            desc = lat_gen.generate_descriptor_entry(instr, func_name)
            latency_descriptors.append(desc)
        
        print(f"  Generated: {len(lat_gen.generated_benchmarks)} latency benchmarks")
        print(f"  Skipped: {len(lat_gen.skipped_instructions)} instructions")

    # Generate throughput benchmarks
    if not args.latency_only:
        print("\n=== Generating Throughput Benchmarks (Phase 1: Arithmetic) ===")
        tput_gen = ThroughputBenchmarkGenerator(config)
        
        for instr in instructions:
            func = tput_gen.generate_benchmark_function(instr)
            if func:
                throughput_functions.append(func)
        
        # Generate descriptors
        for instr, func_name in tput_gen.generated_benchmarks:
            desc = tput_gen.generate_descriptor_entry(instr, func_name)
            throughput_descriptors.append(desc)
        
        print(f"  Generated: {len(tput_gen.generated_benchmarks)} throughput benchmarks")

    # --- Additional Specialized Benchmarks (included by default) ---
    additional_functions = []
    additional_descriptors = []

    # Cache benchmarks
    print("\n=== Generating Cache Benchmarks ===")
    try:
        from .cache_benchmarks import CacheBenchmarkConfig, CacheBenchmarkGenerator
    except ImportError:
        from cache_benchmarks import CacheBenchmarkConfig, CacheBenchmarkGenerator

    cache_config = CacheBenchmarkConfig(
        warmup_iterations=args.warmup,
        measurement_iterations=args.iterations,
        repeat_count=args.repeats
    )
    cache_gen = CacheBenchmarkGenerator(cache_config)
    cache_code, cache_descriptors = cache_gen.generate_all_benchmarks()
    additional_functions.append(cache_code)
    additional_descriptors.extend(cache_descriptors)
    print(f"  Generated: {len(cache_descriptors)} cache benchmarks")

    # Branch Predictor benchmarks
    print("\n=== Generating Branch Predictor Benchmarks ===")
    bp_config = BranchPredictorConfig(
        warmup_iterations=args.warmup,
        measurement_iterations=args.iterations,
        repeat_count=args.repeats,
        loop_iterations=args.loop_iterations
    )
    bp_gen = BranchPredictorBenchmarkGenerator(bp_config)
    bp_functions = bp_gen.generate_all_benchmarks()
    bp_descriptors = bp_gen.generate_all_descriptors()
    additional_functions.extend(bp_functions)
    additional_descriptors.extend(bp_descriptors)
    print(f"  Generated: {len(bp_gen.generated_benchmarks)} branch predictor benchmarks")

    # Structural Hazards benchmark
    print("\n=== Generating Structural Hazard Benchmarks ===")
    hazard_func, hazard_desc = tput_gen.generate_structural_hazard_test()
    additional_functions.append(hazard_func)
    additional_descriptors.append(hazard_desc)
    print(f"  Generated: 1 structural hazard benchmark")

    # Store Buffer benchmarks
    print("\n=== Generating Store Buffer Benchmarks ===")
    sb_functions, sb_descriptors = lat_gen.generate_store_buffer_tests()
    additional_functions.extend(sb_functions)
    additional_descriptors.extend(sb_descriptors)
    print(f"  Generated: {len(sb_descriptors)} store buffer benchmarks")

    # Generate header file
    header_content = generate_header_content(
        latency_functions,
        throughput_functions + additional_functions,
        latency_descriptors,
        throughput_descriptors + additional_descriptors,
        config
    )

    # Write output
    with open(args.output, 'w') as f:
        f.write(header_content)

    total_benchmarks = len(latency_descriptors) + len(throughput_descriptors) + len(additional_descriptors)
    print(f"\n=== Summary ===")
    print(f"Total benchmarks: {total_benchmarks}")
    print(f"  Latency: {len(latency_descriptors)}")
    print(f"  Throughput: {len(throughput_descriptors)}")
    print(f"  Cache: {len(cache_descriptors)}")
    print(f"  Branch Predictor: {len(bp_gen.generated_benchmarks)}")
    print(f"  Structural Hazards: 1")
    print(f"  Store Buffer: {len(sb_descriptors)}")
    print(f"Output written to: {args.output}")


if __name__ == "__main__":
    main()
