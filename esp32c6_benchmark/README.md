# ESP32-C6 Instruction Latency Benchmark

This project measures instruction latencies on the ESP32-C6 RISC-V microcontroller, inspired by the methodology from ["uops.info: Characterizing Latency, Throughput, and Port Usage of Instructions on Intel Microarchitectures"](https://uops.info/) (ASPLOS '19).

## Architecture

The benchmark system is designed to be **modular**:

```
esp32c6_benchmark/
├── CMakeLists.txt              # ESP-IDF project file
├── sdkconfig.defaults          # ESP-IDF configuration
├── main/
│   ├── CMakeLists.txt          # Component build file
│   ├── main.c                  # Benchmark runner (DO NOT MODIFY)
│   ├── benchmark_interface.h   # Interface definition (DO NOT MODIFY)
│   └── generated_benchmarks.h  # Generated by Python script
└── README.md                   # This file
```

### Key Components

1. **`main.c`** - Static benchmark runner that:
   - Reads benchmark definitions from the generated header
   - Runs all benchmarks with warmup and repetitions
   - Outputs results in JSON format (for parsing) and human-readable format
   - **Should NOT be modified** once finalized

2. **`benchmark_interface.h`** - Defines the interface that generated headers must implement:
   - `benchmark_descriptor_t` - Describes each benchmark
   - `benchmark_result_t` - Holds measurement results
   - `BENCHMARKS[]` array and `BENCHMARK_COUNT` - Defined by generated header
   - Helper macros for cycle counting and barriers

3. **`generated_benchmarks.h`** - Generated by Python script, contains:
   - Benchmark function implementations
   - Benchmark descriptor array
   - Configuration (iterations, chain length, etc.)

## How Latency Measurement Works

Following the uops.info methodology, latency is measured by creating **dependency chains**:

```
instruction → instruction → instruction → ... → instruction
     ↓             ↓             ↓                   ↓
   output       input →      output              input →
              becomes                            becomes
```

Each instruction's output register becomes the input for the next instruction. This forces sequential execution, and the total cycles divided by chain length gives the per-instruction latency.

### Example for `add a0, a0, a0`:

```asm
add a0, a0, a0    ; Cycle 1: a0 = a0 + a0
add a0, a0, a0    ; Cycle 2: a0 = a0 + a0 (depends on previous a0)
add a0, a0, a0    ; Cycle 3: ...
... (100 times)
```

Total cycles ÷ 100 = latency per instruction

## Usage

### Prerequisites

- ESP-IDF v5.x installed and configured
- Python 3.8+
- ESP32-C6 development board

### Step 1: Generate Benchmarks

```bash
# From the repository root - uses default paths
python3 benchmark_generator/generate_benchmarks.py

# Or with explicit options
python3 benchmark_generator/generate_benchmarks.py \
    --input isa_extraction/output/esp32c6_instructions.json \
    --output esp32c6_benchmark/main/generated_benchmarks.h \
    --chain-length 100 \
    --iterations 1000 \
    --repeats 5
```

Options:
- `--chain-length`: Number of instructions in dependency chain (default: 100)
- `--iterations`: Number of measurement iterations (default: 1000)
- `--repeats`: Number of times to repeat measurement (default: 5)
- `--warmup`: Warmup iterations before measurement (default: 100)

### Step 2: Build the Project

```bash
cd esp32c6_benchmark

# Set target to ESP32-C6
idf.py set-target esp32c6

# Build
idf.py build
```

### Step 3: Flash and Monitor

```bash
# Flash to device
idf.py flash

# Monitor serial output
idf.py monitor
```

### Step 4: Capture Results

The output includes two formats:

1. **JSON format** (between `=== BEGIN_RESULTS_JSON ===` and `=== END_RESULTS_JSON ===`):
```json
{"instruction":"ADD","asm":"add a0, a0, a0","type":"latency","latency_type":"arithmetic","min_cycles":1,"max_cycles":2,"avg_cycles":1,"iterations":5000,"status":0}
```

2. **Human-readable format**:
```
[  1/100] ADD                  | add a0, a0, a0            | arithmetic
          Cycles: min=1 max=2 avg=1
```

## Extending the Framework

### Current Benchmark Types

The framework includes the following benchmark categories:

1. **Latency**: Measures cycles per instruction using dependency chains (112 benchmarks)
2. **Throughput**: Measures instructions per cycle using independent sequences (125 benchmarks)
3. **Cache Latency**: SRAM, Flash Hit/Miss, Unaligned Load, Instruction Fetch (9 benchmarks)
4. **Branch Predictor**: Backward/Forward taken/not-taken, warmup detection (5 benchmarks)
5. **Structural Hazards**: DIV+ADD overlap test (1 benchmark)
6. **Store Buffer**: Burst throughput and Store-to-Load forwarding (2 benchmarks)

### Adding New Benchmark Types
1. Add new enum values to `benchmark_type_t` in `benchmark_interface.h`
2. Create new generator functions in the Python script
3. The `main.c` runner will automatically handle them

### Custom Instructions

To benchmark custom/vendor-specific instructions:
1. Add them to `esp32c6_instructions.json`
2. Implement special handling in `generate_benchmarks.py` if needed
3. Regenerate the header

## Measurement Accuracy

For accurate measurements:

1. **Disable interrupts/watchdogs** (configured in `sdkconfig.defaults`)
2. **Use maximum CPU frequency** (160 MHz for ESP32-C6)
3. **Warmup phase** to stabilize caches and branch predictors
4. **Multiple repetitions** to filter out outliers
5. **Long dependency chains** to amortize measurement overhead

### Limitations

- **Loads/Stores**: Require memory setup; cache effects matter
- **Branches/Jumps**: Change control flow; need special handling
- **Atomics**: LR/SC pairs need to be tested together
- **System instructions**: Often have side effects, skipped

## Output Format

Results are printed in JSON for easy parsing:

```python
import json

# Parse JSON output
with open('benchmark_results.txt') as f:
    for line in f:
        if line.startswith('{'):
            result = json.loads(line)
            print(f"{result['instruction']}: {result['avg_cycles']} cycles")
```

## License

MIT License - See repository root for details.
